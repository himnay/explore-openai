==========
Prompt 1: |
==========

I am going to give you a streamlit chatbot code. Please load it into the editor, dont modify it.

```
<paste your code here>
```
==========
Prompt 2: |
==========

I want to make the chatbot multimodal. I want to upload the images and ask the llm to describe it
1 - I want the user to submit a form the UI that has the ability to upload the image and the prompt. 
2 - If only prompt is given in the form then invoke the ask_openai function and stream the response as it is today and show it.
  - Also make sure to use client.chat.completions.create() for LLM calls and do not use this api openai.ChatCompletion.create.

```
# Generate response by calling OpenAI.
output = response_generator(prompt)
with st.chat_message("ai"):
    ai_message = st.write_stream(output)

def ask_openai(
    user_question: str,
    temperature: float = 1.0,
    top_p: float = 1.0,
    max_tokens: int = 256,
):
    """Send a user question to OpenAI and stream the completion response."""
    response = client.chat.completions.create(
        model=LLM,
        messages=[
            {"role": "user", "content": user_question},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        stream=True,
    )
    return response
```

3 - If both the prompt and image are passed then call the "ask_openai_image" function.
Please make sure to display the image in the chat conversation window.
If this involves saving the image in the DB then please save it in a separate column in the db. 
I want to have the image being displayed in the convesation window once the user submits the form. 
Please ensure this code generated is not going to give us this warning.

```
The use_column_width parameter has been deprecated and will be removed in a future release. Please utilize the use_container_width parameter instead.

```


```
api_key = os.environ.get("OPENAI_API_KEY")

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


base64_image = encode_image(image_path)

def ask_openai_image(
    user_question: str,
    temperature: float = 1.0,
    top_p: float = 1.0,
    max_tokens: int = 500,
) -> requests.Response:
    print(f"LLM : {LLM}")

    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

    payload = {
        "model": "gpt-4o-mini",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_question},
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                    },
                ],
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
    }

    response = requests.post(
        "https://api.openai.com/v1/chat/completions", headers=headers, json=payload
    )

    print(f"response  type : {type(response)}")
    return response
```


==========
Prompt 3: |
==========

After submitting the form, please do the following:

1 - Provide a spinner for image uploads until we recieve the response from the Open AI call.
2 - User input Prompt still stays in the input box even after the response is recieved from the AI.
 Please clear the prompt from the prompt input box once the form is submited.
 Ensure we are not running into this issue.

 ```
 streamlit.errors.StreamlitAPIException: st.session_state.user_prompt cannot be modified after the widget with key user_prompt is instantiated.

 ```

==========
Prompt 4: |
==========

I want the form to be always at the bottom of the screen. 
Please ensure that the form stays at the bottom in these scenarios.
1 - When the user submits the prompt, the current prompt and the live response that's been rendered from the ai should be at the top of the form.
2 - All the previous conversations in the current chat should also be in the top of the form.